# transformerç ”ä¹ 

## æ‰€æœ‰attentionæ”¾vit_attentionæ–‡ä»¶å¤¹ä¸‹ï¼ŒåŠæ’åŠç”¨ã€‚

### å·²ç»å®žçŽ°

[vsion transformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/Vit.py)

[Swin Tranformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/swin_att.py)

## ç›®å½•
[Q,K,V](https://github.com/Jacky-Android/Study-Vision-transformer/tree/main#qkv)

[QKV Attention](https://github.com/Jacky-Android/Study-Vision-transformer/tree/main#qkv-attention)

[Say Something](https://github.com/Jacky-Android/Study-Vision-transformer/tree/main#say-something)

[Vsion Transformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/tree/main#vision-transformer)

[Swin Tranformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/tree/main#swin-tranformer)

# Q,K,V

Transformeré‡Œé¢çš„Qï¼ŒKï¼ŒVæ˜¯æŒ‡æŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ä¸‰ä¸ªçŸ©é˜µï¼Œå®ƒä»¬éƒ½æ˜¯é€šè¿‡å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚å®ƒä»¬çš„ä½œç”¨æ˜¯å®žçŽ°ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰ï¼Œç”¨äºŽè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ï¼ˆtokenï¼‰ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„è¾“å‡ºã€‚

å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢çŸ©é˜µQç”¨äºŽè¯¢é—®é”®çŸ©é˜µKä¸­çš„å“ªä¸ªtokenä¸ŽæŸ¥è¯¢æœ€ç›¸ä¼¼ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—å¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦åºåˆ—ã€‚ç„¶åŽå¯¹ç›¸ä¼¼åº¦åºåˆ—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå¾—åˆ°ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒï¼Œè¡¨ç¤ºæ¯ä¸ªtokenè¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æœ€åŽï¼Œç”¨è¿™ä¸ªæ¦‚çŽ‡åˆ†å¸ƒå¯¹å€¼çŸ©é˜µVè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„tokenï¼Œè¡¨ç¤ºæŸ¥è¯¢çš„ç»“æžœã€‚

è¿™ç§æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡åž‹å­¦ä¹ åˆ°è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ€§èƒ½ã€‚Transformeræ¨¡åž‹ä¸­ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ˜¯è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆEncoder-Decoder Attentionï¼‰ã€‚

è‡ªæ³¨æ„åŠ›æ˜¯æŒ‡Qï¼ŒKï¼ŒVéƒ½æ¥è‡ªäºŽåŒä¸€ä¸ªè¾“å…¥ï¼Œç”¨äºŽè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¸Žè‡ªèº«çš„ç›¸å…³æ€§ï¼Œä»Žè€Œæ•æ‰è¾“å…¥çš„å†…éƒ¨ç»“æž„ã€‚ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æ˜¯æŒ‡Qæ¥è‡ªäºŽè§£ç å™¨çš„è¾“å‡ºï¼ŒKï¼ŒVæ¥è‡ªäºŽç¼–ç å™¨çš„è¾“å‡ºï¼Œç”¨äºŽè®¡ç®—è§£ç å™¨çš„è¾“å‡ºä¸Žç¼–ç å™¨çš„è¾“å‡ºçš„ç›¸å…³æ€§ï¼Œä»Žè€Œæ•æ‰è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚



![Untitled](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/6d4f831c-073b-4d74-aa79-29b78f478b0e)

## QKV Attention

- é¦–å…ˆï¼Œå°†è¾“å…¥çš„ç‰¹å¾å‘é‡ï¼ˆä¾‹å¦‚è¯å‘é‡ï¼‰åˆ†åˆ«ä¹˜ä»¥ä¸‰ä¸ªä¸åŒçš„å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œå¾—åˆ°æŸ¥è¯¢çŸ©é˜µï¼ˆquery matrixï¼‰Qï¼Œé”®çŸ©é˜µï¼ˆkey matrixï¼‰Kå’Œå€¼çŸ©é˜µï¼ˆvalue matrixï¼‰Vã€‚è¿™ç›¸å½“äºŽå¯¹è¾“å…¥çš„ç‰¹å¾å‘é‡è¿›è¡Œäº†ä¸‰ç§ä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°äº†ä¸‰ç§ä¸åŒçš„è¡¨ç¤ºã€‚
- ç„¶åŽï¼Œè®¡ç®—Qå’ŒKçš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„å¯¹æ•°å€¼ï¼ˆlogitï¼‰ï¼Œå¹¶ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼Œç”¨äºŽè°ƒèŠ‚æ³¨æ„åŠ›çš„æƒé‡ã€‚ç¼©æ”¾å› å­é€šå¸¸æ˜¯Kçš„ç»´åº¦çš„å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œç”¨äºŽé¿å…ç‚¹ç§¯å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå½±å“æ¢¯åº¦çš„ç¨³å®šæ€§ã€‚
- æŽ¥ç€ï¼Œå¯¹æ³¨æ„åŠ›çš„å¯¹æ•°å€¼è¿›è¡Œsoftmaxæ¿€æ´»ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„æƒé‡ï¼ˆweightï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ è¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æ³¨æ„åŠ›çš„æƒé‡æ˜¯ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒï¼Œå®ƒçš„å’Œä¸º1ã€‚
- æœ€åŽï¼Œç”¨æ³¨æ„åŠ›çš„æƒé‡å¯¹Vè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„è¾“å‡ºï¼ˆoutputï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ çš„æ–°çš„è¡¨ç¤ºã€‚æ³¨æ„åŠ›çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŠ æƒå¹³å‡çš„ç»“æžœï¼Œå®ƒçš„ç»´åº¦å’ŒVç›¸åŒã€‚

QKV attention è®¡ç®—å…¬å¼å¯ä»¥ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼š

$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼Œdkæ˜¯Kçš„ç»´åº¦ï¼ŒQKTè¡¨ç¤ºQå’ŒKçš„è½¬ç½®çš„ç‚¹ç§¯ï¼Œsoftmaxå‡½æ•°å°†æ³¨æ„åŠ›çš„å¯¹æ•°å€¼å½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬çš„å’Œä¸º1ã€‚

# Say something

å®žçŽ°ä¸€äº›ç»å…¸çš„vit attention ï¼Œä¹Ÿç®—æ˜¯ç¬”è®°ã€‚ðŸ˜ðŸ˜ðŸ˜

# Vision transformer
![Untitled 1](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/bd8994c9-f6a8-4e97-8a96-3ded0d3551cf)



1) patch embeddingï¼šä¾‹å¦‚è¾“å…¥å›¾ç‰‡å¤§å°ä¸º224x224ï¼Œå°†å›¾ç‰‡åˆ†ä¸ºå›ºå®šå¤§å°çš„patchï¼Œpatchå¤§å°ä¸º16x16ï¼Œåˆ™æ¯å¼ å›¾åƒä¼šç”Ÿæˆ224x224/16x16=196ä¸ªpatchï¼Œå³è¾“å…¥åºåˆ—é•¿åº¦ä¸º**196**ï¼Œæ¯ä¸ªpatchç»´åº¦16x16x3=**768**ï¼Œçº¿æ€§æŠ•å°„å±‚çš„ç»´åº¦ä¸º768xN (N=768)ï¼Œå› æ­¤è¾“å…¥é€šè¿‡çº¿æ€§æŠ•å°„å±‚ä¹‹åŽçš„ç»´åº¦ä¾ç„¶ä¸º196x768ï¼Œå³ä¸€å…±æœ‰196ä¸ªtokenï¼Œæ¯ä¸ªtokençš„ç»´åº¦æ˜¯768ã€‚è¿™é‡Œè¿˜éœ€è¦åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šå­—ç¬¦clsï¼Œå› æ­¤æœ€ç»ˆçš„ç»´åº¦æ˜¯**197x768**ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»é€šè¿‡patch embeddingå°†ä¸€ä¸ªè§†è§‰é—®é¢˜è½¬åŒ–ä¸ºäº†ä¸€ä¸ªseq2seqé—®é¢˜

(2) positional encodingï¼ˆstandard learnable 1D position embeddingsï¼‰ï¼šViTåŒæ ·éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç å¯ä»¥ç†è§£ä¸ºä¸€å¼ è¡¨ï¼Œè¡¨ä¸€å…±æœ‰Nè¡Œï¼ŒNçš„å¤§å°å’Œè¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå‘é‡ï¼Œå‘é‡çš„ç»´åº¦å’Œè¾“å…¥åºåˆ—embeddingçš„ç»´åº¦ç›¸åŒï¼ˆ768ï¼‰ã€‚æ³¨æ„ä½ç½®ç¼–ç çš„æ“ä½œæ˜¯sumï¼Œè€Œä¸æ˜¯concatã€‚åŠ å…¥ä½ç½®ç¼–ç ä¿¡æ¯ä¹‹åŽï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(3) LN/multi-head attention/LNï¼šLNè¾“å‡ºç»´åº¦ä¾ç„¶æ˜¯197x768ã€‚å¤šå¤´è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå…ˆå°†è¾“å…¥æ˜ å°„åˆ°qï¼Œkï¼Œvï¼Œå¦‚æžœåªæœ‰ä¸€ä¸ªå¤´ï¼Œqkvçš„ç»´åº¦éƒ½æ˜¯197x768ï¼Œå¦‚æžœæœ‰12ä¸ªå¤´ï¼ˆ768/12=64ï¼‰ï¼Œåˆ™qkvçš„ç»´åº¦æ˜¯197x64ï¼Œä¸€å…±æœ‰12ç»„qkvï¼Œæœ€åŽå†å°†12ç»„qkvçš„è¾“å‡ºæ‹¼æŽ¥èµ·æ¥ï¼Œè¾“å‡ºç»´åº¦æ˜¯197x768ï¼Œç„¶åŽåœ¨è¿‡ä¸€å±‚LNï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(4) MLPï¼šå°†ç»´åº¦æ”¾å¤§å†ç¼©å°å›žåŽ»ï¼Œ197x768æ”¾å¤§ä¸º197x3072ï¼Œå†ç¼©å°å˜ä¸º**197x768**

## vision transformerçš„attentionå®žçŽ°

```python
class Attention(nn.Module):
    def __init__(self,
                 dim,   # è¾“å…¥tokençš„dim
                 num_heads=8,
                 qkv_bias=False,
                 qk_scale=None,
                 attn_drop_ratio=0.,
                 proj_drop_ratio=0.):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_ratio)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)

    def forward(self, x):
        # [batch_size, num_patches + 1, total_embed_dim]
        B, N, C = x.shape

        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]
        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]
        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]
        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]
        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

# Swin Tranformer

## æ¡†æž¶

![Untitled 2](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/66578fe1-522b-49c8-8496-86af44f48e0d)


- Swin Transformerï¼ˆä¸Šå›¾ä¸º Swin-Tï¼ŒT ä¸º Tinyï¼‰é¦–å…ˆé€šè¿‡è¡¥ä¸åˆ†å‰²æ¨¡å—ï¼ˆå¦‚[ViTï¼‰](https://sh-tsang.medium.com/review-vision-transformer-vit-406568603de0)å°†è¾“å…¥ RGB å›¾åƒåˆ†å‰²ä¸ºä¸é‡å çš„è¡¥ä¸ã€‚
- æ¯ä¸ªè¡¥ä¸éƒ½è¢«è§†ä¸ºä¸€ä¸ªâ€œä»¤ç‰Œâ€ï¼Œå…¶ç‰¹å¾è¢«è®¾ç½®ä¸ºåŽŸå§‹åƒç´  RGB å€¼çš„ä¸²è”ã€‚ä½¿ç”¨**4Ã—4 çš„ patch å¤§å°ï¼Œå› æ­¤æ¯ä¸ª patch çš„ç‰¹å¾ç»´åº¦ä¸º 4Ã—4Ã—3=48**ã€‚çº¿æ€§åµŒå…¥å±‚åº”ç”¨äºŽè¯¥åŽŸå§‹å€¼ç‰¹å¾ï¼Œå°†**å…¶æŠ•å½±åˆ°ä»»æ„ç»´åº¦*C***ã€‚
- ä¸ºäº†äº§ç”Ÿ**åˆ†å±‚è¡¨ç¤º**ï¼Œéšç€ç½‘ç»œå˜å¾—æ›´æ·±ï¼Œé€šè¿‡è¡¥ä¸åˆå¹¶å±‚æ¥å‡å°‘æ ‡è®°çš„æ•°é‡ã€‚ç¬¬ä¸€ä¸ªè¡¥ä¸åˆå¹¶å±‚**è¿žæŽ¥æ¯ç»„ 2Ã—2 ç›¸é‚»è¡¥ä¸çš„ç‰¹å¾ï¼Œå¹¶åœ¨4Â *C*ç»´è¿žæŽ¥ç‰¹å¾**ä¸Šåº”ç”¨çº¿æ€§å±‚ã€‚è¿™**å°†ä»¤ç‰Œæ•°é‡å‡å°‘äº† 2Ã—2 = 4 çš„å€æ•°**ï¼ˆ2 æ¬¡åˆ†è¾¨çŽ‡ä¸‹é‡‡æ ·ï¼‰
- è¾“å‡º**å°ºå¯¸**è®¾ç½®ä¸º**2Â *C**ï¼Œ*åˆ†è¾¨çŽ‡ä¿æŒä¸º***H*Â /8Ã—Â *W*Â /8**ã€‚è¡¥ä¸åˆå¹¶å’Œç‰¹å¾è½¬æ¢çš„ç¬¬ä¸€ä¸ªå—è¢«è¡¨ç¤ºä¸º**â€œé˜¶æ®µ 2â€**ã€‚
- åœ¨æ¯ä¸ª MSA æ¨¡å—å’Œæ¯ä¸ª MLP ä¹‹å‰åº”ç”¨ LayerNorm (LN) å±‚ï¼Œå¹¶åœ¨**æ¯ä¸ª[æ¨¡å—](https://sh-tsang.medium.com/review-layer-normalization-ln-6c2ae88bae47)**ä¹‹åŽåº”ç”¨**æ®‹å·®è¿žæŽ¥ã€‚**

## ****Shifted Window Based Self-Attention****

### ****Window Based Self-Attention (W-MSA)****

![Untitled 3](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/a20ec92e-bcad-4e9c-bfc6-b6a5ffdeb704)


å‡è®¾æ¯ä¸ªçª—å£åŒ…å«***M*Â Ã—Â *M ä¸ª*patch**ï¼Œå…¨å±€ MSA æ¨¡å—å’ŒåŸºäºŽ***h*Â Ã—Â *w*ä¸ªpatchå›¾åƒ**çš„çª—å£çš„è®¡ç®—å¤æ‚åº¦ä¸ºï¼š


![Untitled 4](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/b0b65b30-914b-4108-9a03-6218f3ac766d)

å…¶ä¸­å‰è€…ä¸Žè¡¥ä¸å·*hw*æˆäºŒæ¬¡æ–¹ï¼ŒåŽè€…**åœ¨*M*å›ºå®šï¼ˆé»˜è®¤è®¾ç½®ä¸º 7ï¼‰**æ—¶å‘ˆçº¿æ€§ã€‚

### ****Window Based Self-Attention (W-MSA)****

- åŸºäºŽçª—å£çš„è‡ªæ³¨æ„åŠ›æ¨¡å—**ç¼ºä¹è·¨çª—å£çš„è¿žæŽ¥**ï¼Œè¿™é™åˆ¶äº†å®ƒçš„å»ºæ¨¡èƒ½åŠ›ã€‚
- æå‡ºäº†ä¸€ç§ç§»ä½çª—å£åˆ†åŒºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•**åœ¨è¿žç»­ Swin Transformer å—ä¸­çš„ä¸¤ä¸ªåˆ†åŒºé…ç½®ä¹‹é—´äº¤æ›¿**ã€‚

![Untitled 5](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/813513b2-75e5-476f-8877-ec33899715e6)


â€¢ å…¶ä¸­*zl*Â -1 æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºç‰¹å¾ã€‚

 åœ¨è®¡ç®—ç›¸ä¼¼æ€§æ—¶ï¼Œæ¯ä¸ªå¤´éƒ½åŒ…å«**ç›¸å¯¹ä½ç½®åå·®*Bã€‚***

![Untitled 6](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/5a7910ec-5678-4948-a74c-6476788f7308)


## ç»†èŠ‚

### window_partition

æˆ‘å¤çŽ°çš„ä»£ç å¦‚ä¸‹

- é¦–å…ˆï¼Œå°†è¾“å…¥çš„å›¾åƒå¼ é‡xï¼ˆB, C,H, Wï¼‰æŒ‰ç…§çª—å£å¤§å°window_sizeåˆ’åˆ†ä¸ºï¼ˆB, C,H // window_size, window_size, W // window_size, window_sizeï¼‰çš„å½¢çŠ¶ï¼Œå…¶ä¸­Bæ˜¯æ‰¹é‡å¤§å°ï¼ŒHæ˜¯å›¾åƒé«˜åº¦ï¼ŒWæ˜¯å›¾åƒå®½åº¦ï¼ŒCæ˜¯é€šé“æ•°ã€‚
- ç„¶åŽï¼Œå°†xåœ¨ç¬¬äºŒç»´å’Œç¬¬å››ç»´ä¸Šè¿›è¡Œäº¤æ¢ï¼Œå¾—åˆ°ï¼ˆB, C, W // window_size, H // window_size, window_size, window_sizeï¼‰çš„å½¢çŠ¶ï¼Œè¿™æ ·å¯ä»¥ä¿è¯æ¯ä¸ªçª—å£å†…éƒ¨çš„å…ƒç´ æ˜¯è¿žç»­å­˜å‚¨çš„ã€‚
- æœ€åŽï¼Œå°†xå±•å¹³ä¸ºï¼ˆnum_windows * B, window_size, window_size, Cï¼‰çš„å½¢çŠ¶ï¼Œå…¶ä¸­***num_windows = (H // window_size) * (W // window_size)***æ˜¯æ¯å¼ å›¾åƒåˆ’åˆ†å‡ºçš„çª—å£æ•°é‡ã€‚

è¾“å…¥feature mapä¸º [1,128,32,32] ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º [16,64,128] çš„tokens

```python
def img2windows(img, H_sp, W_sp):
    """
    img: B C H W
    """
    B, C, H, W = img.shape
    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)
     # [batch_size*num_windows, Mh*Mw, total_embed_dim]
    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp * W_sp, C)
    return img_perm

```

### windows attention

å‚ç…§å®˜æ–¹ä»£ç ï¼š

è¿™æ®µä»£ç æ˜¯å®šä¹‰ä¸€ä¸ªåŸºäºŽçª—å£çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç±»ï¼Œå®ƒå¯ä»¥å®žçŽ°shiftedå’Œnon-shiftedä¸¤ç§çª—å£åˆ’åˆ†æ–¹å¼ï¼Œå¹¶ä¸”åŠ å…¥äº†ç›¸å¯¹ä½ç½®åç½®ã€‚å®ƒçš„ä¸»è¦åŠŸèƒ½å’Œå‚æ•°å¦‚ä¸‹ï¼š

- **`__init__`**æ–¹æ³•æ˜¯åˆå§‹åŒ–æ¨¡å—çš„å‚æ•°ï¼ŒåŒ…æ‹¬è¾“å…¥é€šé“æ•°dimï¼Œçª—å£å¤§å°window_sizeï¼Œæ³¨æ„åŠ›å¤´æ•°num_headsï¼Œä»¥åŠä¸€äº›å¯é€‰çš„å‚æ•°ï¼Œå¦‚qkv_biasï¼Œqk_scaleï¼Œattn_dropå’Œproj_dropã€‚è¿™äº›å‚æ•°åˆ†åˆ«æŽ§åˆ¶äº†æ˜¯å¦ç»™æŸ¥è¯¢ã€é”®ã€å€¼æ·»åŠ å¯å­¦ä¹ çš„åç½®ï¼ŒæŸ¥è¯¢å’Œé”®çš„ç¼©æ”¾å› å­ï¼Œæ³¨æ„åŠ›æƒé‡çš„dropoutæ¯”ä¾‹ï¼Œä»¥åŠè¾“å‡ºçš„dropoutæ¯”ä¾‹ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªæ–¹æ³•è¿˜å®šä¹‰äº†ä¸€ä¸ªç›¸å¯¹ä½ç½®åç½®è¡¨relative_position_bias_tableï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å¼ é‡ï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªçª—å£å†…éƒ¨æ¯å¯¹ä½ç½®ä¹‹é—´çš„ç›¸å¯¹ä½ç½®åç½®ã€‚å®ƒçš„å½¢çŠ¶æ˜¯(2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)ï¼Œè¡¨ç¤ºæ¯ä¸ªå¤´æœ‰(2 * window_size[0] - 1) * (2 * window_size[1] - 1)ç§ä¸åŒçš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚è¿™ä¸ªæ–¹æ³•è¿˜æ³¨å†Œäº†ä¸€ä¸ªç¼“å†²åŒºrelative_position_indexï¼Œå®ƒæ˜¯ä¸€ä¸ªå›ºå®šçš„å¼ é‡ï¼Œç”¨æ¥è®°å½•æ¯ä¸ªçª—å£å†…éƒ¨æ¯å¯¹ä½ç½®ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ç´¢å¼•ã€‚å®ƒçš„å½¢çŠ¶æ˜¯window_size[0] * window_size[1], window_size[0] * window_size[1]ï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªçª—å£æœ‰window_size[0] * window_size[1]ä¸ªä½ç½®ï¼Œæ¯ä¸ªä½ç½®ä¸Žå…¶ä»–ä½ç½®ä¹‹é—´æœ‰ä¸€ä¸ªç›¸å¯¹ä½ç½®ç´¢å¼•ï¼ŒèŒƒå›´æ˜¯0åˆ°(2 * window_size[0] - 1) * (2 * window_size[1] - 1) - 1ã€‚
- **`forward`**æ–¹æ³•æ˜¯æ‰§è¡Œæ¨¡å—çš„å‰å‘è®¡ç®—ï¼Œå®ƒæŽ¥å—ä¸¤ä¸ªå‚æ•°ï¼šxå’Œmaskã€‚xæ˜¯è¾“å…¥ç‰¹å¾ï¼Œå®ƒçš„å½¢çŠ¶æ˜¯(num_windows*B, N, C)ï¼Œå…¶ä¸­num_windowsæ˜¯æ¯å¼ å›¾åƒåˆ’åˆ†å‡ºçš„çª—å£æ•°é‡ï¼ŒBæ˜¯æ‰¹é‡å¤§å°ï¼ŒNæ˜¯æ¯ä¸ªçª—å£å†…éƒ¨çš„ä½ç½®æ•°é‡ï¼ˆç­‰äºŽwindow_size[0] * window_size[1]ï¼‰ï¼ŒCæ˜¯é€šé“æ•°ã€‚maskæ˜¯ä¸€ä¸ªå¯é€‰çš„å‚æ•°ï¼Œå®ƒæ˜¯ä¸€ä¸ªæŽ©ç å¼ é‡ï¼Œç”¨æ¥å±è”½ä¸€äº›ä¸éœ€è¦è®¡ç®—æ³¨æ„åŠ›çš„ä½ç½®ã€‚å®ƒçš„å½¢çŠ¶æ˜¯(num_windows, N, N)ï¼Œå…¶ä¸­num_windowså’ŒNä¸Žxç›¸åŒã€‚å¦‚æžœæ²¡æœ‰æä¾›maskï¼Œåˆ™é»˜è®¤ä¸ºNoneã€‚è¿™ä¸ªæ–¹æ³•çš„ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
    - é¦–å…ˆï¼Œè°ƒç”¨self.qkv(x)å¾—åˆ°æŸ¥è¯¢ã€é”®ã€å€¼ä¸‰ä¸ªå¼ é‡ï¼Œå¹¶å°†å®ƒä»¬é‡å¡‘ä¸º(B_, N, 3, self.num_heads, C // self.num_heads)çš„å½¢çŠ¶ï¼Œå¹¶åœ¨ç¬¬ä¸€ç»´å’Œç¬¬ä¸‰ç»´ä¸Šè¿›è¡Œäº¤æ¢ï¼Œå¾—åˆ°(3, B_, self.num_heads, N, C // self.num_heads)çš„å½¢çŠ¶ã€‚ç„¶åŽå°†è¿™ä¸ªå¼ é‡åˆ†è§£ä¸ºq, k, vä¸‰ä¸ªå¼ é‡ï¼Œåˆ†åˆ«è¡¨ç¤ºæŸ¥è¯¢ã€é”®ã€å€¼ã€‚
    - ç„¶åŽï¼Œå°†qä¹˜ä»¥self.scaleå¾—åˆ°ç¼©æ”¾åŽçš„æŸ¥è¯¢ï¼Œå¹¶ä¸Žkè¿›è¡Œè½¬ç½®çŸ©é˜µä¹˜æ³•å¾—åˆ°æ³¨æ„åŠ›å¾—åˆ†å¼ é‡attnã€‚å®ƒçš„å½¢çŠ¶æ˜¯(B_, self.num_heads, N, N)ï¼Œè¡¨ç¤ºæ¯ä¸ªå¤´æ¯ä¸ªçª—å£å†…éƒ¨æ¯å¯¹ä½ç½®ä¹‹é—´çš„æ³¨æ„åŠ›å¾—åˆ†ã€‚
    - æŽ¥ç€ï¼Œæ ¹æ®self.relative_position_indexä»Žself.relative_position_bias_tableä¸­å–å‡ºç›¸åº”çš„ç›¸å¯¹ä½ç½®åç½®ï¼Œå¹¶å°†å…¶é‡å¡‘ä¸º(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)çš„å½¢çŠ¶ï¼Œå¹¶åœ¨ç¬¬ä¸€ç»´å’Œç¬¬ä¸‰ç»´ä¸Šè¿›è¡Œäº¤æ¢ï¼Œå¾—åˆ°(-1, self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1])çš„å½¢çŠ¶ã€‚ç„¶åŽå°†å…¶åœ¨ç¬¬é›¶ç»´ä¸Šå¢žåŠ ä¸€ä¸ªç»´åº¦ï¼Œå¹¶ä¸Žattnç›¸åŠ å¾—åˆ°åŠ å…¥äº†ç›¸å¯¹ä½ç½®åç½®çš„æ³¨æ„åŠ›å¾—åˆ†å¼ é‡attnã€‚
    - ç„¶åŽï¼Œå¦‚æžœæä¾›äº†maskï¼Œåˆ™å°†attné‡å¡‘ä¸º(B_ // nW, nW, self.num_heads, N, N)çš„å½¢çŠ¶ï¼Œå¹¶åœ¨ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´ä¸Šå¢žåŠ ä¸€ä¸ªç»´åº¦ï¼Œä¸Žmaskç›¸åŠ å¾—åˆ°å±è”½äº†ä¸€äº›ä½ç½®çš„æ³¨æ„åŠ›å¾—åˆ†å¼ é‡attnã€‚ç„¶åŽå°†å…¶é‡å¡‘ä¸º(-1, self.num_heads, N, N)çš„å½¢çŠ¶ã€‚å¦‚æžœæ²¡æœ‰æä¾›maskï¼Œåˆ™ç›´æŽ¥å°†attné€šè¿‡self.softmaxå‡½æ•°å¾—åˆ°æ³¨æ„åŠ›æƒé‡å¼ é‡attnï¼Œå¹¶å¯¹å…¶è¿›è¡Œself.attn_dropæ“ä½œã€‚
    - æœ€åŽï¼Œå°†attnä¸Žvè¿›è¡ŒçŸ©é˜µä¹˜æ³•å¾—åˆ°è¾“å‡ºç‰¹å¾xï¼Œå¹¶å°†å…¶åœ¨ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´ä¸Šè¿›è¡Œäº¤æ¢ï¼Œå¹¶é‡å¡‘ä¸º(B_, N, C)çš„å½¢çŠ¶ã€‚ç„¶åŽé€šè¿‡self.proj(x)å¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºç‰¹å¾xï¼Œå¹¶å¯¹å…¶è¿›è¡Œself.proj_dropæ“ä½œã€‚è¿”å›žxä½œä¸ºæ¨¡å—çš„è¾“å‡ºã€‚

![Untitled 7](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/4b714b78-f898-4fdb-863f-a6afb7fdf04c)

![Untitled 8](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/c86eb124-9d94-464b-9976-274c93550ddd)

```python
class WindowAttention(nn.Module):
	def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

    super().__init__()
    self.dim = dim
    self.window_size = window_size  # Wh, Ww
    self.num_heads = num_heads
    head_dim = dim // num_heads
    self.scale = qk_scale or head_dim ** -0.5

    # define a parameter table of relative position bias
    self.relative_position_bias_table = nn.Parameter(
        torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

    # get pair-wise relative position index for each token inside the window
    coords_h = torch.arange(self.window_size[0])
    coords_w = torch.arange(self.window_size[1])
    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
    relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
    relative_coords[:, :, 1] += self.window_size[1] - 1
    relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
    relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
    self.register_buffer("relative_position_index", relative_position_index)

    self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
    self.attn_drop = nn.Dropout(attn_drop)
    self.proj = nn.Linear(dim, dim)
    self.proj_drop = nn.Dropout(proj_drop)

    trunc_normal_(self.relative_position_bias_table, std=.02)
    self.softmax = nn.Softmax(dim=-1)

def forward(self, x, mask=None):
    """
    Args:
        x: input features with shape of (num_windows*B, N, C)
        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
    """
    B_, N, C = x.shape
    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
    q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

    q = q * self.scale
    attn = (q @ k.transpose(-2, -1))

    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
        self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
    attn = attn + relative_position_bias.unsqueeze(0)

    if mask is not None:
        nW = mask.shape[0]
        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
        attn = attn.view(-1, self.num_heads, N, N)
        attn = self.softmax(attn)
    else:
        attn = self.softmax(attn)

    attn = self.attn_drop(attn)

    x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
    x = self.proj(x)
    x = self.proj_drop(x)
    return x
```

### window_reverse

å°†ä¸€ä¸ªä¸ªwindowè¿˜åŽŸæˆä¸€ä¸ªfeature map

- é¦–å…ˆï¼Œæ ¹æ®H_spå’ŒW_spè®¡ç®—å‡ºæ¯å¼ å›¾åƒåˆ’åˆ†å‡ºçš„çª—å£æ•°é‡ï¼Œå³H * W / H_sp / W_spï¼Œå¹¶ç”¨å®ƒé™¤ä»¥Bâ€™å¾—åˆ°æ‰¹é‡å¤§å°Bã€‚
- ç„¶åŽï¼Œå°†è¾“å…¥å¼ é‡img_splits_hwé‡å¡‘ä¸º(B, H // H_sp, W // W_sp, H_sp, W_sp, C)çš„å½¢çŠ¶ï¼Œå…¶ä¸­æ¯ä¸ªç»´åº¦åˆ†åˆ«è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼Œçª—å£è¡Œæ•°ï¼Œçª—å£åˆ—æ•°ï¼Œçª—å£é«˜åº¦ï¼Œçª—å£å®½åº¦å’Œé€šé“æ•°ã€‚
- ç„¶åŽï¼Œå°†imgåœ¨ç¬¬äºŒç»´å’Œç¬¬å››ç»´ä¸Šè¿›è¡Œäº¤æ¢ï¼Œå¾—åˆ°(B, W // W_sp, H // H_sp, H_sp, W_sp, C)çš„å½¢çŠ¶ï¼Œå¹¶å°†å…¶å±•å¹³ä¸º(B, C, H, W)çš„å½¢çŠ¶ã€‚è¿™æ ·å°±å®žçŽ°äº†å°†æ¯ä¸ªçª—å£å†…éƒ¨çš„å…ƒç´ æŒ‰ç…§åŽŸå§‹å›¾åƒçš„é¡ºåºæŽ’åˆ—ï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„å›¾åƒã€‚

```python
def windows2img(img_splits_hw, H_sp, W_sp, H, W):
    """
    img_splits_hw: B' H W C
    """
    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))

    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)
    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, img.shape[-1],H,W)
    return img
```

> [å›¾è§£Swin Transformer - çŸ¥ä¹Ž (zhihu.com)](https://zhuanlan.zhihu.com/p/367111046)
>

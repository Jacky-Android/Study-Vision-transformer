# transformerç ”ä¹ 

## æ‰€æœ‰attentionæ”¾vit_attentionæ–‡ä»¶å¤¹ä¸‹ï¼ŒåŠæ’åŠç”¨ã€‚
### å·²ç»å®ç°
[vsion transformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/Vit.py)

# Q,K,V

Transformeré‡Œé¢çš„Qï¼ŒKï¼ŒVæ˜¯æŒ‡æŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ä¸‰ä¸ªçŸ©é˜µï¼Œå®ƒä»¬éƒ½æ˜¯é€šè¿‡å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚å®ƒä»¬çš„ä½œç”¨æ˜¯å®ç°ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰ï¼Œç”¨äºè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ï¼ˆtokenï¼‰ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„è¾“å‡ºã€‚

å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢çŸ©é˜µQç”¨äºè¯¢é—®é”®çŸ©é˜µKä¸­çš„å“ªä¸ªtokenä¸æŸ¥è¯¢æœ€ç›¸ä¼¼ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—å¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦åºåˆ—ã€‚ç„¶åå¯¹ç›¸ä¼¼åº¦åºåˆ—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå¾—åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºæ¯ä¸ªtokenè¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æœ€åï¼Œç”¨è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒå¯¹å€¼çŸ©é˜µVè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„tokenï¼Œè¡¨ç¤ºæŸ¥è¯¢çš„ç»“æœã€‚

è¿™ç§æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡å‹å­¦ä¹ åˆ°è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ€§èƒ½ã€‚Transformeræ¨¡å‹ä¸­ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ˜¯è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆEncoder-Decoder Attentionï¼‰ã€‚

è‡ªæ³¨æ„åŠ›æ˜¯æŒ‡Qï¼ŒKï¼ŒVéƒ½æ¥è‡ªäºåŒä¸€ä¸ªè¾“å…¥ï¼Œç”¨äºè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¸è‡ªèº«çš„ç›¸å…³æ€§ï¼Œä»è€Œæ•æ‰è¾“å…¥çš„å†…éƒ¨ç»“æ„ã€‚ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æ˜¯æŒ‡Qæ¥è‡ªäºè§£ç å™¨çš„è¾“å‡ºï¼ŒKï¼ŒVæ¥è‡ªäºç¼–ç å™¨çš„è¾“å‡ºï¼Œç”¨äºè®¡ç®—è§£ç å™¨çš„è¾“å‡ºä¸ç¼–ç å™¨çš„è¾“å‡ºçš„ç›¸å…³æ€§ï¼Œä»è€Œæ•æ‰è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

![Untitled](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/ed7507ab-98b1-447b-9055-9a90ae2fdd86)


## QKV Attention

- é¦–å…ˆï¼Œå°†è¾“å…¥çš„ç‰¹å¾å‘é‡ï¼ˆä¾‹å¦‚è¯å‘é‡ï¼‰åˆ†åˆ«ä¹˜ä»¥ä¸‰ä¸ªä¸åŒçš„å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œå¾—åˆ°æŸ¥è¯¢çŸ©é˜µï¼ˆquery matrixï¼‰Qï¼Œé”®çŸ©é˜µï¼ˆkey matrixï¼‰Kå’Œå€¼çŸ©é˜µï¼ˆvalue matrixï¼‰Vã€‚è¿™ç›¸å½“äºå¯¹è¾“å…¥çš„ç‰¹å¾å‘é‡è¿›è¡Œäº†ä¸‰ç§ä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°äº†ä¸‰ç§ä¸åŒçš„è¡¨ç¤ºã€‚
- ç„¶åï¼Œè®¡ç®—Qå’ŒKçš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„å¯¹æ•°å€¼ï¼ˆlogitï¼‰ï¼Œå¹¶ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼Œç”¨äºè°ƒèŠ‚æ³¨æ„åŠ›çš„æƒé‡ã€‚ç¼©æ”¾å› å­é€šå¸¸æ˜¯Kçš„ç»´åº¦çš„å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œç”¨äºé¿å…ç‚¹ç§¯å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå½±å“æ¢¯åº¦çš„ç¨³å®šæ€§ã€‚
- æ¥ç€ï¼Œå¯¹æ³¨æ„åŠ›çš„å¯¹æ•°å€¼è¿›è¡Œsoftmaxæ¿€æ´»ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„æƒé‡ï¼ˆweightï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ è¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æ³¨æ„åŠ›çš„æƒé‡æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒçš„å’Œä¸º1ã€‚
- æœ€åï¼Œç”¨æ³¨æ„åŠ›çš„æƒé‡å¯¹Vè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„è¾“å‡ºï¼ˆoutputï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ çš„æ–°çš„è¡¨ç¤ºã€‚æ³¨æ„åŠ›çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŠ æƒå¹³å‡çš„ç»“æœï¼Œå®ƒçš„ç»´åº¦å’ŒVç›¸åŒã€‚

QKV attention è®¡ç®—å…¬å¼å¯ä»¥ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼š

$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼Œdkæ˜¯Kçš„ç»´åº¦ï¼ŒQKTè¡¨ç¤ºQå’ŒKçš„è½¬ç½®çš„ç‚¹ç§¯ï¼Œsoftmaxå‡½æ•°å°†æ³¨æ„åŠ›çš„å¯¹æ•°å€¼å½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬çš„å’Œä¸º1ã€‚

# Say something

å®ç°ä¸€äº›ç»å…¸çš„vit attention ï¼Œä¹Ÿç®—æ˜¯ç¬”è®°ã€‚ğŸ˜ğŸ˜ğŸ˜

# Vision transformer
paper:[An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)

![Untitled 1](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/93183f78-7a1c-46e9-a4d9-05577ad3fe22)


(1) patch embeddingï¼šä¾‹å¦‚è¾“å…¥å›¾ç‰‡å¤§å°ä¸º224x224ï¼Œå°†å›¾ç‰‡åˆ†ä¸ºå›ºå®šå¤§å°çš„patchï¼Œpatchå¤§å°ä¸º16x16ï¼Œåˆ™æ¯å¼ å›¾åƒä¼šç”Ÿæˆ224x224/16x16=196ä¸ªpatchï¼Œå³è¾“å…¥åºåˆ—é•¿åº¦ä¸º**196**ï¼Œæ¯ä¸ªpatchç»´åº¦16x16x3=**768**ï¼Œçº¿æ€§æŠ•å°„å±‚çš„ç»´åº¦ä¸º768xN (N=768)ï¼Œå› æ­¤è¾“å…¥é€šè¿‡çº¿æ€§æŠ•å°„å±‚ä¹‹åçš„ç»´åº¦ä¾ç„¶ä¸º196x768ï¼Œå³ä¸€å…±æœ‰196ä¸ªtokenï¼Œæ¯ä¸ªtokençš„ç»´åº¦æ˜¯768ã€‚è¿™é‡Œè¿˜éœ€è¦åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šå­—ç¬¦clsï¼Œå› æ­¤æœ€ç»ˆçš„ç»´åº¦æ˜¯**197x768**ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»é€šè¿‡patch embeddingå°†ä¸€ä¸ªè§†è§‰é—®é¢˜è½¬åŒ–ä¸ºäº†ä¸€ä¸ªseq2seqé—®é¢˜

(2) positional encodingï¼ˆstandard learnable 1D position embeddingsï¼‰ï¼šViTåŒæ ·éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç å¯ä»¥ç†è§£ä¸ºä¸€å¼ è¡¨ï¼Œè¡¨ä¸€å…±æœ‰Nè¡Œï¼ŒNçš„å¤§å°å’Œè¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå‘é‡ï¼Œå‘é‡çš„ç»´åº¦å’Œè¾“å…¥åºåˆ—embeddingçš„ç»´åº¦ç›¸åŒï¼ˆ768ï¼‰ã€‚æ³¨æ„ä½ç½®ç¼–ç çš„æ“ä½œæ˜¯sumï¼Œè€Œä¸æ˜¯concatã€‚åŠ å…¥ä½ç½®ç¼–ç ä¿¡æ¯ä¹‹åï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(3) LN/multi-head attention/LNï¼šLNè¾“å‡ºç»´åº¦ä¾ç„¶æ˜¯197x768ã€‚å¤šå¤´è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå…ˆå°†è¾“å…¥æ˜ å°„åˆ°qï¼Œkï¼Œvï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªå¤´ï¼Œqkvçš„ç»´åº¦éƒ½æ˜¯197x768ï¼Œå¦‚æœæœ‰12ä¸ªå¤´ï¼ˆ768/12=64ï¼‰ï¼Œåˆ™qkvçš„ç»´åº¦æ˜¯197x64ï¼Œä¸€å…±æœ‰12ç»„qkvï¼Œæœ€åå†å°†12ç»„qkvçš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œè¾“å‡ºç»´åº¦æ˜¯197x768ï¼Œç„¶ååœ¨è¿‡ä¸€å±‚LNï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(4) MLPï¼šå°†ç»´åº¦æ”¾å¤§å†ç¼©å°å›å»ï¼Œ197x768æ”¾å¤§ä¸º197x3072ï¼Œå†ç¼©å°å˜ä¸º**197x768**

## vision transformerçš„attentionå®ç°
[vsion transformer attention](https://github.com/Jacky-Android/Study-Vision-transformer/blob/main/vit_attention/Vit.py)
```python
class Attention(nn.Module):
    def __init__(self,
                 dim,   # è¾“å…¥tokençš„dim
                 num_heads=8,
                 qkv_bias=False,
                 qk_scale=None,
                 attn_drop_ratio=0.,
                 proj_drop_ratio=0.):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_ratio)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)

    def forward(self, x):
        # [batch_size, num_patches + 1, total_embed_dim]
        B, N, C = x.shape

        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]
        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]
        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]
        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]
        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```
# Swin Tranformer

## æ¡†æ¶

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2/43e90388-9d71-4270-bca1-b90b4dc217d9/Untitled.png)

- Swin Transformerï¼ˆä¸Šå›¾ä¸º Swin-Tï¼ŒT ä¸º Tinyï¼‰é¦–å…ˆé€šè¿‡è¡¥ä¸åˆ†å‰²æ¨¡å—ï¼ˆå¦‚[ViTï¼‰](https://sh-tsang.medium.com/review-vision-transformer-vit-406568603de0)å°†è¾“å…¥ RGB å›¾åƒåˆ†å‰²ä¸ºä¸é‡å çš„è¡¥ä¸ã€‚
- æ¯ä¸ªè¡¥ä¸éƒ½è¢«è§†ä¸ºä¸€ä¸ªâ€œä»¤ç‰Œâ€ï¼Œå…¶ç‰¹å¾è¢«è®¾ç½®ä¸ºåŸå§‹åƒç´  RGB å€¼çš„ä¸²è”ã€‚ä½¿ç”¨**4Ã—4 çš„ patch å¤§å°ï¼Œå› æ­¤æ¯ä¸ª patch çš„ç‰¹å¾ç»´åº¦ä¸º 4Ã—4Ã—3=48**ã€‚çº¿æ€§åµŒå…¥å±‚åº”ç”¨äºè¯¥åŸå§‹å€¼ç‰¹å¾ï¼Œå°†**å…¶æŠ•å½±åˆ°ä»»æ„ç»´åº¦*C***ã€‚
- ä¸ºäº†äº§ç”Ÿ**åˆ†å±‚è¡¨ç¤º**ï¼Œéšç€ç½‘ç»œå˜å¾—æ›´æ·±ï¼Œé€šè¿‡è¡¥ä¸åˆå¹¶å±‚æ¥å‡å°‘æ ‡è®°çš„æ•°é‡ã€‚ç¬¬ä¸€ä¸ªè¡¥ä¸åˆå¹¶å±‚**è¿æ¥æ¯ç»„ 2Ã—2 ç›¸é‚»è¡¥ä¸çš„ç‰¹å¾ï¼Œå¹¶åœ¨4Â *C*ç»´è¿æ¥ç‰¹å¾**ä¸Šåº”ç”¨çº¿æ€§å±‚ã€‚è¿™**å°†ä»¤ç‰Œæ•°é‡å‡å°‘äº† 2Ã—2 = 4 çš„å€æ•°**ï¼ˆ2 æ¬¡åˆ†è¾¨ç‡ä¸‹é‡‡æ ·ï¼‰
- è¾“å‡º**å°ºå¯¸**è®¾ç½®ä¸º**2Â *C**ï¼Œ*åˆ†è¾¨ç‡ä¿æŒä¸º***H*Â /8Ã—Â *W*Â /8**ã€‚è¡¥ä¸åˆå¹¶å’Œç‰¹å¾è½¬æ¢çš„ç¬¬ä¸€ä¸ªå—è¢«è¡¨ç¤ºä¸º**â€œé˜¶æ®µ 2â€**ã€‚
- åœ¨æ¯ä¸ª MSA æ¨¡å—å’Œæ¯ä¸ª MLP ä¹‹å‰åº”ç”¨ LayerNorm (LN) å±‚ï¼Œå¹¶åœ¨**æ¯ä¸ª[æ¨¡å—](https://sh-tsang.medium.com/review-layer-normalization-ln-6c2ae88bae47)**ä¹‹ååº”ç”¨**æ®‹å·®è¿æ¥ã€‚**

## ****Shifted Window Based Self-Attention****

### ****Window Based Self-Attention (W-MSA)****

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2/3f0b06b0-513c-4de2-b388-f7e2f8642e5b/Untitled.png)

å‡è®¾æ¯ä¸ªçª—å£åŒ…å«***M*Â Ã—Â *M ä¸ª*patch**ï¼Œå…¨å±€ MSA æ¨¡å—å’ŒåŸºäº***h*Â Ã—Â *w*ä¸ªpatchå›¾åƒ**çš„çª—å£çš„è®¡ç®—å¤æ‚åº¦ä¸ºï¼š

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2/7ec378fa-a36f-44e3-809f-a1efc460ef6f/Untitled.png)

å…¶ä¸­å‰è€…ä¸è¡¥ä¸å·*hw*æˆäºŒæ¬¡æ–¹ï¼Œåè€…**åœ¨*M*å›ºå®šï¼ˆé»˜è®¤è®¾ç½®ä¸º 7ï¼‰**æ—¶å‘ˆçº¿æ€§ã€‚

### ****Window Based Self-Attention (W-MSA)****

- åŸºäºçª—å£çš„è‡ªæ³¨æ„åŠ›æ¨¡å—**ç¼ºä¹è·¨çª—å£çš„è¿æ¥**ï¼Œè¿™é™åˆ¶äº†å®ƒçš„å»ºæ¨¡èƒ½åŠ›ã€‚
- æå‡ºäº†ä¸€ç§ç§»ä½çª—å£åˆ†åŒºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•**åœ¨è¿ç»­ Swin Transformer å—ä¸­çš„ä¸¤ä¸ªåˆ†åŒºé…ç½®ä¹‹é—´äº¤æ›¿**ã€‚

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f3e18bd5-e2f1-4629-b637-ae7fa3ad4ab2/956e0376-aff5-4f8e-964c-67284a7ecef5/Untitled.png)

â€¢ å…¶ä¸­*zl*Â -1 æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºç‰¹å¾ã€‚

 åœ¨è®¡ç®—ç›¸ä¼¼æ€§æ—¶ï¼Œæ¯ä¸ªå¤´éƒ½åŒ…å«**ç›¸å¯¹ä½ç½®åå·®*Bã€‚***

[å›¾è§£Swin Transformer - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/367111046)

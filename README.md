# transformerç ”ä¹ 

# Q,K,V

Transformeré‡Œé¢çš„Qï¼ŒKï¼ŒVæ˜¯æŒ‡æŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ä¸‰ä¸ªçŸ©é˜µï¼Œå®ƒä»¬éƒ½æ˜¯é€šè¿‡å¯¹è¾“å…¥è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°çš„ã€‚å®ƒä»¬çš„ä½œç”¨æ˜¯å®žçŽ°ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰ï¼Œç”¨äºŽè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ï¼ˆtokenï¼‰ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„è¾“å‡ºã€‚

å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢çŸ©é˜µQç”¨äºŽè¯¢é—®é”®çŸ©é˜µKä¸­çš„å“ªä¸ªtokenä¸ŽæŸ¥è¯¢æœ€ç›¸ä¼¼ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—å¾—åˆ°ä¸€ä¸ªç›¸ä¼¼åº¦åºåˆ—ã€‚ç„¶åŽå¯¹ç›¸ä¼¼åº¦åºåˆ—è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå¾—åˆ°ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒï¼Œè¡¨ç¤ºæ¯ä¸ªtokenè¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æœ€åŽï¼Œç”¨è¿™ä¸ªæ¦‚çŽ‡åˆ†å¸ƒå¯¹å€¼çŸ©é˜µVè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„tokenï¼Œè¡¨ç¤ºæŸ¥è¯¢çš„ç»“æžœã€‚

è¿™ç§æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è®©æ¨¡åž‹å­¦ä¹ åˆ°è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»Žè€Œæé«˜æ¨¡åž‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ€§èƒ½ã€‚Transformeræ¨¡åž‹ä¸­ä½¿ç”¨äº†ä¸¤ç§ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«æ˜¯è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆEncoder-Decoder Attentionï¼‰ã€‚

è‡ªæ³¨æ„åŠ›æ˜¯æŒ‡Qï¼ŒKï¼ŒVéƒ½æ¥è‡ªäºŽåŒä¸€ä¸ªè¾“å…¥ï¼Œç”¨äºŽè®¡ç®—è¾“å…¥çš„æ¯ä¸ªå…ƒç´ ä¸Žè‡ªèº«çš„ç›¸å…³æ€§ï¼Œä»Žè€Œæ•æ‰è¾“å…¥çš„å†…éƒ¨ç»“æž„ã€‚ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æ˜¯æŒ‡Qæ¥è‡ªäºŽè§£ç å™¨çš„è¾“å‡ºï¼ŒKï¼ŒVæ¥è‡ªäºŽç¼–ç å™¨çš„è¾“å‡ºï¼Œç”¨äºŽè®¡ç®—è§£ç å™¨çš„è¾“å‡ºä¸Žç¼–ç å™¨çš„è¾“å‡ºçš„ç›¸å…³æ€§ï¼Œä»Žè€Œæ•æ‰è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚

![Untitled](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/ed7507ab-98b1-447b-9055-9a90ae2fdd86)


## QKV Attention

- é¦–å…ˆï¼Œå°†è¾“å…¥çš„ç‰¹å¾å‘é‡ï¼ˆä¾‹å¦‚è¯å‘é‡ï¼‰åˆ†åˆ«ä¹˜ä»¥ä¸‰ä¸ªä¸åŒçš„å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œå¾—åˆ°æŸ¥è¯¢çŸ©é˜µï¼ˆquery matrixï¼‰Qï¼Œé”®çŸ©é˜µï¼ˆkey matrixï¼‰Kå’Œå€¼çŸ©é˜µï¼ˆvalue matrixï¼‰Vã€‚è¿™ç›¸å½“äºŽå¯¹è¾“å…¥çš„ç‰¹å¾å‘é‡è¿›è¡Œäº†ä¸‰ç§ä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°äº†ä¸‰ç§ä¸åŒçš„è¡¨ç¤ºã€‚
- ç„¶åŽï¼Œè®¡ç®—Qå’ŒKçš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„å¯¹æ•°å€¼ï¼ˆlogitï¼‰ï¼Œå¹¶ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­ï¼ˆscaleï¼‰ï¼Œç”¨äºŽè°ƒèŠ‚æ³¨æ„åŠ›çš„æƒé‡ã€‚ç¼©æ”¾å› å­é€šå¸¸æ˜¯Kçš„ç»´åº¦çš„å¹³æ–¹æ ¹çš„å€’æ•°ï¼Œç”¨äºŽé¿å…ç‚¹ç§¯å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå½±å“æ¢¯åº¦çš„ç¨³å®šæ€§ã€‚
- æŽ¥ç€ï¼Œå¯¹æ³¨æ„åŠ›çš„å¯¹æ•°å€¼è¿›è¡Œsoftmaxæ¿€æ´»ï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„æƒé‡ï¼ˆweightï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ è¢«æ³¨æ„çš„ç¨‹åº¦ã€‚æ³¨æ„åŠ›çš„æƒé‡æ˜¯ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒï¼Œå®ƒçš„å’Œä¸º1ã€‚
- æœ€åŽï¼Œç”¨æ³¨æ„åŠ›çš„æƒé‡å¯¹Vè¿›è¡ŒåŠ æƒå’Œï¼Œå¾—åˆ°æ³¨æ„åŠ›çš„è¾“å‡ºï¼ˆoutputï¼‰ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥å…ƒç´ çš„æ–°çš„è¡¨ç¤ºã€‚æ³¨æ„åŠ›çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŠ æƒå¹³å‡çš„ç»“æžœï¼Œå®ƒçš„ç»´åº¦å’ŒVç›¸åŒã€‚

QKV attention è®¡ç®—å…¬å¼å¯ä»¥ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºä¸ºï¼š

$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$

å…¶ä¸­ï¼Œdkæ˜¯Kçš„ç»´åº¦ï¼ŒQKTè¡¨ç¤ºQå’ŒKçš„è½¬ç½®çš„ç‚¹ç§¯ï¼Œsoftmaxå‡½æ•°å°†æ³¨æ„åŠ›çš„å¯¹æ•°å€¼å½’ä¸€åŒ–ï¼Œä½¿å¾—å®ƒä»¬çš„å’Œä¸º1ã€‚

# Say something

å®žçŽ°ä¸€äº›ç»å…¸çš„vit attention ï¼Œä¹Ÿç®—æ˜¯ç¬”è®°ã€‚ðŸ˜ðŸ˜ðŸ˜

# Vision transformer
paper:[An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)

![Untitled 1](https://github.com/Jacky-Android/Study-Vision-transformer/assets/55181594/93183f78-7a1c-46e9-a4d9-05577ad3fe22)


(1) patch embeddingï¼šä¾‹å¦‚è¾“å…¥å›¾ç‰‡å¤§å°ä¸º224x224ï¼Œå°†å›¾ç‰‡åˆ†ä¸ºå›ºå®šå¤§å°çš„patchï¼Œpatchå¤§å°ä¸º16x16ï¼Œåˆ™æ¯å¼ å›¾åƒä¼šç”Ÿæˆ224x224/16x16=196ä¸ªpatchï¼Œå³è¾“å…¥åºåˆ—é•¿åº¦ä¸º**196**ï¼Œæ¯ä¸ªpatchç»´åº¦16x16x3=**768**ï¼Œçº¿æ€§æŠ•å°„å±‚çš„ç»´åº¦ä¸º768xN (N=768)ï¼Œå› æ­¤è¾“å…¥é€šè¿‡çº¿æ€§æŠ•å°„å±‚ä¹‹åŽçš„ç»´åº¦ä¾ç„¶ä¸º196x768ï¼Œå³ä¸€å…±æœ‰196ä¸ªtokenï¼Œæ¯ä¸ªtokençš„ç»´åº¦æ˜¯768ã€‚è¿™é‡Œè¿˜éœ€è¦åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šå­—ç¬¦clsï¼Œå› æ­¤æœ€ç»ˆçš„ç»´åº¦æ˜¯**197x768**ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»é€šè¿‡patch embeddingå°†ä¸€ä¸ªè§†è§‰é—®é¢˜è½¬åŒ–ä¸ºäº†ä¸€ä¸ªseq2seqé—®é¢˜

(2) positional encodingï¼ˆstandard learnable 1D position embeddingsï¼‰ï¼šViTåŒæ ·éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç å¯ä»¥ç†è§£ä¸ºä¸€å¼ è¡¨ï¼Œè¡¨ä¸€å…±æœ‰Nè¡Œï¼ŒNçš„å¤§å°å’Œè¾“å…¥åºåˆ—é•¿åº¦ç›¸åŒï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå‘é‡ï¼Œå‘é‡çš„ç»´åº¦å’Œè¾“å…¥åºåˆ—embeddingçš„ç»´åº¦ç›¸åŒï¼ˆ768ï¼‰ã€‚æ³¨æ„ä½ç½®ç¼–ç çš„æ“ä½œæ˜¯sumï¼Œè€Œä¸æ˜¯concatã€‚åŠ å…¥ä½ç½®ç¼–ç ä¿¡æ¯ä¹‹åŽï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(3) LN/multi-head attention/LNï¼šLNè¾“å‡ºç»´åº¦ä¾ç„¶æ˜¯197x768ã€‚å¤šå¤´è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå…ˆå°†è¾“å…¥æ˜ å°„åˆ°qï¼Œkï¼Œvï¼Œå¦‚æžœåªæœ‰ä¸€ä¸ªå¤´ï¼Œqkvçš„ç»´åº¦éƒ½æ˜¯197x768ï¼Œå¦‚æžœæœ‰12ä¸ªå¤´ï¼ˆ768/12=64ï¼‰ï¼Œåˆ™qkvçš„ç»´åº¦æ˜¯197x64ï¼Œä¸€å…±æœ‰12ç»„qkvï¼Œæœ€åŽå†å°†12ç»„qkvçš„è¾“å‡ºæ‹¼æŽ¥èµ·æ¥ï¼Œè¾“å‡ºç»´åº¦æ˜¯197x768ï¼Œç„¶åŽåœ¨è¿‡ä¸€å±‚LNï¼Œç»´åº¦ä¾ç„¶æ˜¯**197x768**

(4) MLPï¼šå°†ç»´åº¦æ”¾å¤§å†ç¼©å°å›žåŽ»ï¼Œ197x768æ”¾å¤§ä¸º197x3072ï¼Œå†ç¼©å°å˜ä¸º**197x768**

## vision transformerçš„attentionå®žçŽ°

```python
class Attention(nn.Module):
    def __init__(self,
                 dim,   # è¾“å…¥tokençš„dim
                 num_heads=8,
                 qkv_bias=False,
                 qk_scale=None,
                 attn_drop_ratio=0.,
                 proj_drop_ratio=0.):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop_ratio)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)

    def forward(self, x):
        # [batch_size, num_patches + 1, total_embed_dim]
        B, N, C = x.shape

        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]
        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]
        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]
        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]
        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]
        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

[å›¾è§£Swin Transformer - çŸ¥ä¹Ž (zhihu.com)](https://zhuanlan.zhihu.com/p/367111046)
